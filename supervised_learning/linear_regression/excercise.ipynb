{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is excercise on regression and it covers\n",
    "1. Finding the cost\n",
    "2. finding gradient descent\n",
    "3. feature scaling\n",
    "4. predict for a dataset that's not seen before or classify the dataset for training and for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.244e+03 3.000e+00 1.000e+00 6.400e+01]\n",
      " [1.947e+03 3.000e+00 2.000e+00 1.700e+01]\n",
      " [1.725e+03 3.000e+00 2.000e+00 4.200e+01]\n",
      " [1.959e+03 3.000e+00 2.000e+00 1.500e+01]\n",
      " [1.314e+03 2.000e+00 1.000e+00 1.400e+01]\n",
      " [8.640e+02 2.000e+00 1.000e+00 6.600e+01]\n",
      " [1.836e+03 3.000e+00 1.000e+00 1.700e+01]\n",
      " [1.026e+03 3.000e+00 1.000e+00 4.300e+01]\n",
      " [3.194e+03 4.000e+00 2.000e+00 8.700e+01]\n",
      " [7.880e+02 2.000e+00 1.000e+00 8.000e+01]\n",
      " [1.200e+03 2.000e+00 2.000e+00 1.700e+01]\n",
      " [1.557e+03 2.000e+00 1.000e+00 1.800e+01]\n",
      " [1.430e+03 3.000e+00 1.000e+00 2.000e+01]\n",
      " [1.220e+03 2.000e+00 1.000e+00 1.500e+01]\n",
      " [1.092e+03 2.000e+00 1.000e+00 6.400e+01]\n",
      " [8.480e+02 1.000e+00 1.000e+00 1.700e+01]\n",
      " [1.682e+03 3.000e+00 2.000e+00 2.300e+01]\n",
      " [1.768e+03 3.000e+00 2.000e+00 1.800e+01]\n",
      " [1.040e+03 3.000e+00 1.000e+00 4.400e+01]\n",
      " [1.652e+03 2.000e+00 1.000e+00 2.100e+01]\n",
      " [1.088e+03 2.000e+00 1.000e+00 3.500e+01]\n",
      " [1.316e+03 3.000e+00 1.000e+00 1.400e+01]\n",
      " [1.593e+03 0.000e+00 1.000e+00 2.000e+01]\n",
      " [9.720e+02 2.000e+00 1.000e+00 7.300e+01]\n",
      " [1.097e+03 3.000e+00 1.000e+00 3.700e+01]\n",
      " [1.004e+03 2.000e+00 1.000e+00 5.100e+01]\n",
      " [9.040e+02 3.000e+00 1.000e+00 5.500e+01]\n",
      " [1.694e+03 3.000e+00 1.000e+00 1.300e+01]\n",
      " [1.073e+03 2.000e+00 1.000e+00 1.000e+02]\n",
      " [1.419e+03 3.000e+00 2.000e+00 1.900e+01]\n",
      " [1.164e+03 3.000e+00 1.000e+00 5.200e+01]\n",
      " [1.935e+03 3.000e+00 2.000e+00 1.200e+01]\n",
      " [1.216e+03 2.000e+00 2.000e+00 7.400e+01]\n",
      " [2.482e+03 4.000e+00 2.000e+00 1.600e+01]\n",
      " [1.200e+03 2.000e+00 1.000e+00 1.800e+01]\n",
      " [1.840e+03 3.000e+00 2.000e+00 2.000e+01]\n",
      " [1.851e+03 3.000e+00 2.000e+00 5.700e+01]\n",
      " [1.660e+03 3.000e+00 2.000e+00 1.900e+01]\n",
      " [1.096e+03 2.000e+00 2.000e+00 9.700e+01]\n",
      " [1.775e+03 3.000e+00 2.000e+00 2.800e+01]\n",
      " [2.030e+03 4.000e+00 2.000e+00 4.500e+01]\n",
      " [1.784e+03 4.000e+00 2.000e+00 1.070e+02]\n",
      " [1.073e+03 2.000e+00 1.000e+00 1.000e+02]\n",
      " [1.552e+03 3.000e+00 1.000e+00 1.600e+01]\n",
      " [1.953e+03 3.000e+00 2.000e+00 1.600e+01]\n",
      " [1.224e+03 2.000e+00 2.000e+00 1.200e+01]\n",
      " [1.616e+03 3.000e+00 1.000e+00 1.600e+01]\n",
      " [8.160e+02 2.000e+00 1.000e+00 5.800e+01]\n",
      " [1.349e+03 3.000e+00 1.000e+00 2.100e+01]\n",
      " [1.571e+03 3.000e+00 1.000e+00 1.400e+01]\n",
      " [1.486e+03 3.000e+00 1.000e+00 5.700e+01]\n",
      " [1.506e+03 2.000e+00 1.000e+00 1.600e+01]\n",
      " [1.097e+03 3.000e+00 1.000e+00 2.700e+01]\n",
      " [1.764e+03 3.000e+00 2.000e+00 2.400e+01]\n",
      " [1.208e+03 2.000e+00 1.000e+00 1.400e+01]\n",
      " [1.470e+03 3.000e+00 2.000e+00 2.400e+01]\n",
      " [1.768e+03 3.000e+00 2.000e+00 8.400e+01]\n",
      " [1.654e+03 3.000e+00 1.000e+00 1.900e+01]\n",
      " [1.029e+03 3.000e+00 1.000e+00 6.000e+01]\n",
      " [1.120e+03 2.000e+00 2.000e+00 1.600e+01]\n",
      " [1.150e+03 3.000e+00 1.000e+00 6.200e+01]\n",
      " [8.160e+02 2.000e+00 1.000e+00 3.900e+01]\n",
      " [1.040e+03 3.000e+00 1.000e+00 2.500e+01]\n",
      " [1.392e+03 3.000e+00 1.000e+00 6.400e+01]\n",
      " [1.603e+03 3.000e+00 2.000e+00 2.900e+01]\n",
      " [1.215e+03 3.000e+00 1.000e+00 6.300e+01]\n",
      " [1.073e+03 2.000e+00 1.000e+00 1.000e+02]\n",
      " [2.599e+03 4.000e+00 2.000e+00 2.200e+01]\n",
      " [1.431e+03 3.000e+00 1.000e+00 5.900e+01]\n",
      " [2.090e+03 3.000e+00 2.000e+00 2.600e+01]\n",
      " [1.790e+03 4.000e+00 2.000e+00 4.900e+01]\n",
      " [1.484e+03 3.000e+00 2.000e+00 1.600e+01]\n",
      " [1.040e+03 3.000e+00 1.000e+00 2.500e+01]\n",
      " [1.431e+03 3.000e+00 1.000e+00 2.200e+01]\n",
      " [1.159e+03 3.000e+00 1.000e+00 5.300e+01]\n",
      " [1.547e+03 3.000e+00 2.000e+00 1.200e+01]\n",
      " [1.983e+03 3.000e+00 2.000e+00 2.200e+01]\n",
      " [1.056e+03 3.000e+00 1.000e+00 5.300e+01]\n",
      " [1.180e+03 2.000e+00 1.000e+00 9.900e+01]\n",
      " [1.358e+03 2.000e+00 1.000e+00 1.700e+01]\n",
      " [9.600e+02 3.000e+00 1.000e+00 5.100e+01]\n",
      " [1.456e+03 3.000e+00 2.000e+00 1.600e+01]\n",
      " [1.446e+03 3.000e+00 2.000e+00 2.500e+01]\n",
      " [1.208e+03 2.000e+00 1.000e+00 1.500e+01]\n",
      " [1.553e+03 3.000e+00 2.000e+00 1.600e+01]\n",
      " [8.820e+02 3.000e+00 1.000e+00 4.900e+01]\n",
      " [2.030e+03 4.000e+00 2.000e+00 4.500e+01]\n",
      " [1.040e+03 3.000e+00 1.000e+00 6.200e+01]\n",
      " [1.616e+03 3.000e+00 1.000e+00 1.600e+01]\n",
      " [8.030e+02 2.000e+00 1.000e+00 8.000e+01]\n",
      " [1.430e+03 3.000e+00 2.000e+00 2.100e+01]\n",
      " [1.656e+03 3.000e+00 1.000e+00 6.100e+01]\n",
      " [1.541e+03 3.000e+00 1.000e+00 1.600e+01]\n",
      " [9.480e+02 3.000e+00 1.000e+00 5.300e+01]\n",
      " [1.224e+03 2.000e+00 2.000e+00 1.200e+01]\n",
      " [1.432e+03 2.000e+00 1.000e+00 4.300e+01]\n",
      " [1.660e+03 3.000e+00 2.000e+00 1.900e+01]\n",
      " [1.212e+03 3.000e+00 1.000e+00 2.000e+01]\n",
      " [1.050e+03 2.000e+00 1.000e+00 6.500e+01]] [300.    509.8   394.    540.    415.    230.    560.    294.    718.2\n",
      " 200.    302.    468.    374.2   388.    282.    311.8   401.    449.8\n",
      " 301.    502.    340.    400.282 572.    264.    304.    298.    219.8\n",
      " 490.7   216.96  368.2   280.    526.87  237.    562.426 369.8   460.\n",
      " 374.    390.    158.    426.    390.    277.774 216.96  425.8   504.\n",
      " 329.    464.    220.    358.    478.    334.    426.98  290.    463.\n",
      " 390.8   354.    350.    460.    237.    288.304 282.    249.    304.\n",
      " 332.    351.8   310.    216.96  666.336 330.    480.    330.3   348.\n",
      " 304.    384.    316.    430.4   450.    284.    275.    414.    258.\n",
      " 378.    350.    412.    373.    225.    390.    267.4   464.    174.\n",
      " 340.    430.    440.    216.    329.    388.    390.    356.    257.8  ]\n"
     ]
    }
   ],
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:,:4]\n",
    "    y = data[:,4]\n",
    "    return X, y\n",
    "X,y = load_house_data()\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 4)\n",
      "(99,)\n",
      "First training data [1.244e+03 3.000e+00 1.000e+00 6.400e+01]\n",
      "Price of first training data 300.0\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) # 4 features and 99 trainig examples\n",
    "print(y.shape) # price of 99 training examples\n",
    "\n",
    "print(\"First training data\",X[0])\n",
    "print(\"Price of first training data\",y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does cost mean\n",
    "# sum of all cost = f_yi - yi ** 2 / m\n",
    "def compute_cost (X,y,w,b):\n",
    "    # m,n = X.shape\n",
    "    # cost = 0.0\n",
    "    # for i in range(m):\n",
    "    #     f_wb_i = w * X[i] + b if n == 1 else np.dot(X[i],w) + b\n",
    "    #     cost += (f_wb_i - y[i]) ** 2\n",
    "    # total_cost = 1 / (2 * m * cost)\n",
    "    # return total_cost\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i],w) + b       \n",
    "        cost = cost + (f_wb_i - y[i])**2              \n",
    "    cost = cost/(2*m)                                 \n",
    "    return(np.squeeze(cost)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.245"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot([18.145],[0]) + 1.245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next step is to compute gradient descent and update w and b until convergence\n",
    "- So how do you compute w and b\n",
    "- The best w and b makes cost zero\n",
    "- So we need to find the derivative of dj/dw and dj/db, J being the cost function\n",
    "- what is J = 1/2*sum((X[i]*W[i] + b - y[i]) ** 2)\n",
    "- if we assume error = X[i]*W[i] + b - y[i]\n",
    "- J will be J = (error) ** 2\n",
    "- Now, by the chain rule finding dj/derr and derr/dw  and multiplying will give dj/dw\n",
    "- So, what is dj/derr = 2 * err (power rule of derivative) and what is derr/dw = X[i] (product rule of derivative)\n",
    "- Therefore dj/dw = err * x[i][j]\n",
    "- and with the same approch dj/db = err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,w,b):\n",
    "    m,n = X.shape\n",
    "    # dj_dw = np.zeros(n) # weight of each feature\n",
    "    dj_dw = 0.0 if n == 1 else np.zeros(n) # weight of each feature\n",
    "    dj_db = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i],w) + b\n",
    "        err = f_wb_i - y[i]\n",
    "        if n > 1:\n",
    "            for j in range(n):\n",
    "                dj_dw[j] += (err * X[i][j])\n",
    "        else: dj_dw += err * X[i] \n",
    "        dj_db += err\n",
    "    return dj_db/m,dj_dw/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so since dj_dw and dj_db tells us in which direction we should go\n",
    "# let's update w and b with according to dj_dw and dj_db\n",
    "def gradient_descent (X,y,w_init,b_init,iteration,alpha):\n",
    "    w = copy.deepcopy(w_init)\n",
    "    b = b_init\n",
    "    for i in range(iteration):\n",
    "        dj_db,dj_dw = compute_gradient(X,y,w,b)\n",
    "        w = w - (alpha * dj_dw)\n",
    "        b = b - (alpha * dj_db)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(X,y,w,b)\n",
    "            print(f\"Cost at {i} iterations is: {cost}:\")\n",
    "        # cost = compute_cost(X,y,w,b)\n",
    "        # print(f\"Cost at {i} iterations is: {cost}:\")\n",
    "    print(\"final result of w\",w,\"final result of b\",b)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at 0 iterations is: 44231.26525678279:\n",
      "Cost at 1 iterations is: 27646.11540274138:\n",
      "Cost at 2 iterations is: 17510.224442092407:\n",
      "Cost at 3 iterations is: 11315.74324982964:\n",
      "Cost at 4 iterations is: 7530.0211455750805:\n",
      "Cost at 5 iterations is: 5216.3916908889905:\n",
      "Cost at 6 iterations is: 3802.419151959339:\n",
      "Cost at 7 iterations is: 2938.2642947777135:\n",
      "Cost at 8 iterations is: 2410.1258445063145:\n",
      "Cost at 9 iterations is: 2087.340968967634:\n",
      "final result of w [2.31283016e-01 4.17834784e-04 2.12498494e-04 4.80830049e-03] final result of b 0.00015472928633378463\n"
     ]
    }
   ],
   "source": [
    "# iteration = 1001\n",
    "iteration = 10\n",
    "# alpha = 1e-7\n",
    "alpha = 1e-7\n",
    "w_init = np.zeros(X.shape[1])\n",
    "b_init = 0.0\n",
    "\n",
    "w,b = gradient_descent(X,y,w_init,b_init,iteration,alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction 288.0254243852543 actual price 300.0\n",
      "prediction 450.39160730114634 actual price 509.8\n",
      "prediction 399.16698516907104 actual price 394.0\n",
      "prediction 453.1573868971556 actual price 540.0\n",
      "prediction 303.97440267499803 actual price 415.0\n",
      "prediction 200.14707691333479 actual price 230.0\n",
      "prediction 424.7189799804646 actual price 560.0\n",
      "prediction 237.50475249621127 actual price 294.0\n",
      "prediction 739.1385276412537 actual price 718.2\n",
      "prediction 182.63688387259882 actual price 200.0\n",
      "prediction 277.62277620353206 actual price 302.0\n",
      "prediction 360.1954088660797 actual price 468.0\n",
      "prediction 330.8325002170018 actual price 374.2\n",
      "prediction 282.2386074323761 actual price 388.0\n",
      "prediction 252.86998805522282 actual price 282.0\n",
      "prediction 196.21052409178537 actual price 311.8\n",
      "prediction 389.13045775383046 actual price 401.0\n",
      "prediction 408.9967556631555 actual price 449.8\n",
      "prediction 240.74752302652877 actual price 301.0\n",
      "prediction 382.1817203270857 actual price 502.0\n",
      "prediction 251.80541527529027 actual price 340.0\n",
      "prediction 304.4373865426138 actual price 400.282\n",
      "prediction 368.53037838847644 actual price 572.0\n",
      "prediction 225.1593007897182 actual price 264.0\n",
      "prediction 253.89699685880277 actual price 304.0\n",
      "prediction 232.45457470420948 actual price 298.0\n",
      "prediction 209.34592409935047 actual price 219.8\n",
      "prediction 391.85755844740964 actual price 490.7\n",
      "prediction 248.64870956102897 actual price 216.96\n",
      "prediction 328.2837912344267 actual price 368.2\n",
      "prediction 269.4650834660619 actual price 280.0\n",
      "prediction 447.59216960169306 actual price 526.87\n",
      "prediction 281.597377594234 actual price 237.0\n",
      "prediction 574.1236306180542 actual price 562.426\n",
      "prediction 277.62737200553 actual price 369.8\n",
      "prediction 425.6587494460991 actual price 460.0\n",
      "prediction 428.3807697448804 actual price 374.0\n",
      "prediction 384.0229981907081 actual price 390.0\n",
      "prediction 253.95400653561734 actual price 158.0\n",
      "prediction 410.6638197829883 actual price 426.0\n",
      "prediction 469.7231479122428 actual price 390.0\n",
      "prediction 413.12564050438505 actual price 277.774\n",
      "prediction 248.64870956102897 actual price 216.96\n",
      "prediction 359.0297950177987 actual price 425.8\n",
      "prediction 451.77449709915095 actual price 504.0\n",
      "prediction 283.1495270950586 actual price 329.0\n",
      "prediction 373.8319080684294 actual price 464.0\n",
      "prediction 189.00702572142566 actual price 220.0\n",
      "prediction 312.10338418778923 actual price 358.0\n",
      "prediction 363.4145557287207 actual price 478.0\n",
      "prediction 343.9622562545079 actual price 334.0\n",
      "prediction 348.39035842787433 actual price 426.98\n",
      "prediction 253.84891385388272 actual price 290.0\n",
      "prediction 408.1004734004431 actual price 463.0\n",
      "prediction 279.45840293489084 actual price 390.8\n",
      "prediction 340.1032665741081 actual price 354.0\n",
      "prediction 409.31410349562776 actual price 350.0\n",
      "prediction 382.63508759371746 actual price 460.0\n",
      "prediction 238.28034265382365 actual price 237.0\n",
      "prediction 259.1153265897516 actual price 288.304\n",
      "prediction 266.2752042411565 actual price 282.0\n",
      "prediction 188.9156680120776 actual price 249.0\n",
      "prediction 240.6561653171807 actual price 304.0\n",
      "prediction 322.25531081483797 actual price 332.0\n",
      "prediction 370.88794925991016 actual price 351.8\n",
      "prediction 281.3134086086953 actual price 310.0\n",
      "prediction 248.64870956102897 actual price 216.96\n",
      "prediction 601.2125933416906 actual price 666.336\n",
      "prediction 331.25130695260606 actual price 330.0\n",
      "prediction 483.5083533530775 actual price 480.0\n",
      "prediction 414.23445717434544 actual price 330.3\n",
      "prediction 343.3027623999975 actual price 348.0\n",
      "prediction 240.6561653171807 actual price 304.0\n",
      "prediction 331.0733998344019 actual price 384.0\n",
      "prediction 268.31347668447336 actual price 316.0\n",
      "prediction 357.8543592322441 actual price 430.4\n",
      "prediction 458.7418373945862 actual price 450.0\n",
      "prediction 244.49132599361448 actual price 284.0\n",
      "prediction 273.39118401706025 actual price 275.0\n",
      "prediction 314.1652802987827 actual price 414.0\n",
      "prediction 222.27853981668437 actual price 258.0\n",
      "prediction 336.82683794034654 actual price 378.0\n",
      "prediction 334.55728248061354 actual price 350.0\n",
      "prediction 279.46321123538286 actual price 412.0\n",
      "prediction 359.26129053270876 actual price 373.0\n",
      "prediction 204.22884793524412 actual price 225.0\n",
      "prediction 469.7231479122428 actual price 390.0\n",
      "prediction 240.83407243538483 actual price 267.4\n",
      "prediction 373.8319080684294 actual price 464.0\n",
      "prediction 186.10612911884039 actual price 174.0\n",
      "prediction 330.8375210159878 actual price 340.0\n",
      "prediction 383.29960224721384 actual price 430.0\n",
      "prediction 356.4856818372215 actual price 440.0\n",
      "prediction 219.5127602206751 actual price 216.0\n",
      "prediction 283.1495270950586 actual price 329.0\n",
      "prediction 331.4052393263666 actual price 388.0\n",
      "prediction 384.0229981907081 actual price 390.0\n",
      "prediction 280.41280263829077 actual price 356.0\n",
      "prediction 243.16090966623838 actual price 257.8\n"
     ]
    }
   ],
   "source": [
    "m = X.shape[0]\n",
    "for i in range(m):\n",
    "    p = np.dot(X[i],w) + b\n",
    "    print(\"prediction\",p, \"actual price\",y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Prediction is not so bad but it's also not so close to the target value\n",
    "- Also, the cost is not decreasing by much after the 100th iteration.\n",
    "- And that's because sqft feature w/c is X[i][0] is much larger than the other features\n",
    "- So we need to use feature scaling to optimize that data\n",
    "- And see if we can find a lower cost and accurate prediction than the previous iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalization (X):\n",
    "    mean = np.mean(X,axis=0) # mean of the col, mean ill have shape (n,)\n",
    "    sigma = np.std(X,axis=0) # the standard deviation of each column/feature\n",
    "    X_norm = (X - mean)/sigma\n",
    "    return X_norm,mean,sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mu = [1.41837374e+03 2.71717172e+00 1.38383838e+00 3.83838384e+01], \n",
      "X_sigma = [411.61562893   0.65196523   0.48631932  25.77788069]\n",
      "Peak to Peak range by column in Raw X:[2.406e+03 4.000e+00 1.000e+00 9.500e+01]\n",
      "Peak to Peak range by column in Normalized X:[5.8452591  6.13529646 2.05626214 3.68533012]\n"
     ]
    }
   ],
   "source": [
    "X_norm,mean,sigma = z_score_normalization(X)\n",
    "print(f\"X_mu = {mean}, \\nX_sigma = {sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw X:{np.ptp(X,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")\n",
    "# After normalization feature[0] or sqft maximum is down to almost 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at 0 iterations is: 57617.03252196659:\n",
      "Cost at 100 iterations is: 221.0856266404719:\n",
      "Cost at 200 iterations is: 219.20890028987435:\n",
      "Cost at 300 iterations is: 219.2068266767102:\n",
      "Cost at 400 iterations is: 219.20682438502703:\n",
      "Cost at 500 iterations is: 219.20682438249423:\n",
      "Cost at 600 iterations is: 219.20682438249156:\n",
      "Cost at 700 iterations is: 219.20682438249133:\n",
      "Cost at 800 iterations is: 219.20682438249162:\n",
      "Cost at 900 iterations is: 219.2068243824914:\n",
      "final result of w [110.56039756 -21.26715096 -32.70718139 -37.97015909] final result of b 363.15608080808056\n"
     ]
    }
   ],
   "source": [
    "# now let's train our model with the normalized input data\n",
    "alpha = 1.0e-1\n",
    "iteration=1000\n",
    "w_norm,b_norm = gradient_descent(X_norm,y,w_init,b_init,iteration,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled features get very accurate results much, much faster!. Notice the gradient of each parameter is tiny by the end of this fairly short run. A learning rate of 0.1 is a good start for regression with normalized features. Let's plot our predictions versus the target values. Note, the prediction is made using the normalized feature while the plot is shown using the original feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction 295.176153014491 actual price 300.0\n",
      "prediction 485.9779633156195 actual price 509.8\n",
      "prediction 389.52416547712676 actual price 394.0\n",
      "prediction 492.14712499067645 actual price 540.0\n",
      "prediction 420.2470182541521 actual price 415.0\n",
      "prediction 222.78186731046634 actual price 230.0\n",
      "prediction 523.417783476759 actual price 560.0\n",
      "prediction 267.55358319014005 actual price 294.0\n",
      "prediction 685.1952161268126 actual price 718.2\n",
      "prediction 181.74654412954288 actual price 200.0\n",
      "prediction 317.9530344921217 actual price 302.0\n",
      "prediction 479.62518007591024 actual price 468.0\n",
      "prediction 409.94682655710835 actual price 374.2\n",
      "prediction 393.52554342739234 actual price 388.0\n",
      "prediction 286.96885968636377 actual price 282.0\n",
      "prediction 323.28006626784327 actual price 311.8\n",
      "prediction 405.96083370483524 actual price 401.0\n",
      "prediction 436.42539770380677 actual price 449.8\n",
      "prediction 269.8410237138005 actual price 301.0\n",
      "prediction 500.7233582540422 actual price 502.0\n",
      "prediction 328.61071386010894 actual price 340.0\n",
      "prediction 388.1641513733624 actual price 400.282\n",
      "prediction 551.5890077015238 actual price 572.0\n",
      "prediction 241.47996165668377 actual price 264.0\n",
      "prediction 295.4621055973917 actual price 304.0\n",
      "prediction 282.48063327636623 actual price 298.0\n",
      "prediction 217.1085597368257 actual price 219.8\n",
      "prediction 491.16832952900495 actual price 490.7\n",
      "prediction 228.83835994304854 actual price 216.96\n",
      "prediction 341.21065052188817 actual price 368.2\n",
      "prediction 291.36376071979726 actual price 280.0\n",
      "prediction 490.11962264476523 actual price 526.87\n",
      "prediction 238.29110911827874 actual price 237.0\n",
      "prediction 598.5324401396865 actual price 562.426\n",
      "prediction 383.73459877868794 actual price 369.8\n",
      "prediction 452.8187257457432 actual price 460.0\n",
      "prediction 401.2732836150396 actual price 374.0\n",
      "prediction 405.9435079242147 actual price 390.0\n",
      "prediction 172.18056908019318 actual price 158.0\n",
      "prediction 423.5758608885046 actual price 426.0\n",
      "prediction 434.4084989308338 actual price 390.0\n",
      "prediction 277.00822157955685 actual price 277.774\n",
      "prediction 228.83835994304854 actual price 216.96\n",
      "prediction 448.6080545770481 actual price 425.8\n",
      "prediction 489.06254415314794 actual price 504.0\n",
      "prediction 331.7643322714075 actual price 329.0\n",
      "prediction 465.7985229328527 actual price 464.0\n",
      "prediction 221.67281147698753 actual price 220.0\n",
      "prediction 386.71716561512136 actual price 358.0\n",
      "prediction 456.6574237285212 actual price 478.0\n",
      "prediction 370.4884324890798 actual price 334.0\n",
      "prediction 468.87247446322215 actual price 426.98\n",
      "prediction 310.19184988910996 actual price 290.0\n",
      "prediction 426.513146856538 actual price 463.0\n",
      "prediction 391.7753050398508 actual price 390.8\n",
      "prediction 347.5444328470608 actual price 354.0\n",
      "prediction 339.2090853784663 actual price 350.0\n",
      "prediction 471.58644023159616 actual price 460.0\n",
      "prediction 243.31882109839736 actual price 237.0\n",
      "prediction 297.9379234765378 actual price 288.304\n",
      "prediction 272.8736014752467 actual price 282.0\n",
      "prediction 249.65932563125222 actual price 249.0\n",
      "prediction 297.8275378680652 actual price 304.0\n",
      "prediction 334.9291110872891 actual price 332.0\n",
      "prediction 375.903502753108 actual price 351.8\n",
      "prediction 288.8596964699389 actual price 310.0\n",
      "prediction 228.83835994304854 actual price 216.96\n",
      "prediction 621.1209185276107 actual price 666.336\n",
      "prediction 352.7694248874666 actual price 330.0\n",
      "prediction 511.1311461855738 actual price 480.0\n",
      "prediction 364.0523448798794 actual price 330.3\n",
      "prediction 363.08864323326765 actual price 348.0\n",
      "prediction 297.8275378680652 actual price 304.0\n",
      "prediction 407.26947876682414 actual price 384.0\n",
      "prediction 288.54778095032816 actual price 316.0\n",
      "prediction 385.90240823770006 actual price 430.4\n",
      "prediction 488.2827296199004 actual price 450.0\n",
      "prediction 260.8818709402052 actual price 284.0\n",
      "prediction 259.05164865458113 actual price 275.0\n",
      "prediction 427.6465419612523 actual price 414.0\n",
      "prediction 238.042117264842 actual price 258.0\n",
      "prediction 355.5678133276032 actual price 378.0\n",
      "prediction 339.6250327844623 actual price 350.0\n",
      "prediction 390.30233061067895 actual price 412.0\n",
      "prediction 381.62211692936944 actual price 373.0\n",
      "prediction 220.03718281454886 actual price 225.0\n",
      "prediction 434.4084989308338 actual price 390.0\n",
      "prediction 243.32748398870763 actual price 267.4\n",
      "prediction 465.7985229328527 actual price 464.0\n",
      "prediction 185.77556015043456 actual price 174.0\n",
      "prediction 341.2193134121984 actual price 340.0\n",
      "prediction 410.25871634249836 actual price 430.0\n",
      "prediction 445.6534428283942 actual price 440.0\n",
      "prediction 231.872955589785 actual price 216.0\n",
      "prediction 331.7643322714075 actual price 329.0\n",
      "prediction 409.2256858391838 actual price 388.0\n",
      "prediction 405.9435079242147 actual price 390.0\n",
      "prediction 351.39179372014905 actual price 356.0\n",
      "prediction 274.2146403986952 actual price 257.8\n"
     ]
    }
   ],
   "source": [
    "m = X_norm.shape[0]\n",
    "for i in range(m):\n",
    "    p = np.dot(X_norm[i],w_norm) + b_norm\n",
    "    print(\"prediction\",p, \"actual price\",y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction** The point of generating our model is to use it to predict housing prices that are not in the data set. Let's predict the price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old. Recall, that you must normalize the data with the mean and standard deviation derived when the training data was normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53052829  0.43380884 -0.78927234  0.06269567]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "318.7090923199992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_house = np.array([1200, 3, 1, 40])\n",
    "# you should normalize with mean and standard deviation that you get from trainig data, you don't normalize by taking the mean and \n",
    "# standard deviation of prediction data itself b/c it doesn't make sense \n",
    "x_house_norm = (x_house - mean)/sigma\n",
    "print(x_house_norm)\n",
    "prediction = np.dot(x_house_norm,w_norm) + b_norm\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [0.]\n",
      "Cost at 0 iterations is: 1657.5632937500002:\n",
      "Cost at 100 iterations is: 694.5491067092513:\n",
      "Cost at 200 iterations is: 588.4754483004803:\n",
      "Cost at 300 iterations is: 526.4137606474569:\n",
      "Cost at 400 iterations is: 490.10264314551796:\n",
      "Cost at 500 iterations is: 468.8576963175959:\n",
      "Cost at 600 iterations is: 456.42768047748433:\n",
      "Cost at 700 iterations is: 449.1551137331723:\n",
      "Cost at 800 iterations is: 444.9000727315512:\n",
      "Cost at 900 iterations is: 442.41052886595054:\n",
      "final result of w [18.69806954] final result of b -52.08341025448667\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0, 20, 1).reshape(-1,1)\n",
    "y = 1 + x**2\n",
    "y = y.reshape(20,)\n",
    "\n",
    "w = np.zeros(x.shape[1])\n",
    "print(\"w\",w)\n",
    "b = 0.0\n",
    "iteration = 1000\n",
    "alpha = 1e-2\n",
    "\n",
    "w_model,b_model = gradient_descent(x,y,w,b,iteration,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as expected, not a great fit. What is needed is something like y = wx**2 + b, or a polynomial feature. To accomplish this, you can modify the input data to engineer the needed features. If you swap the original data with a version that squares the x value, then you can achieve y = wx**2 + b,\n",
    ". Let's try it. Swap X for X**2 below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,20,1).reshape(-1,1)\n",
    "y = 1 + x**2\n",
    "\n",
    "#  Engineer features \n",
    "X = x**2      #<-- added engineered feature\n",
    "iteration = 10000\n",
    "alpha=1e-5\n",
    "w_model,b_model = gradient_descent(X,y,w,b,iteration,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! near perfect fit. Notice the values of \n",
    " and b printed right above the graph: w,b found by gradient descent: w: [1.], b: 0.0490. Gradient descent modified our initial values of \n",
    " to be (1.0,0.049) or a model of \n",
    ", very close to our target of \n",
    ". If you ran it longer, it could be a better match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we knew that an x**2 term was required. It may not always be obvious which features are required. One could add a variety of potential features to try and find the most useful. For example, what if we had instead tried : y = wx1 + wx2**2 + y = wx2**3\n",
    " ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# engineer features .\n",
    "X = np.c_[x, x**2, x**3]   #<-- added engineered feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at 0 iterations is: 1140.2924039588138:\n",
      "Cost at 100 iterations is: 378.84024762689825:\n",
      "Cost at 200 iterations is: 372.8909233165143:\n",
      "Cost at 300 iterations is: 367.035031236854:\n",
      "Cost at 400 iterations is: 361.27110406458786:\n",
      "Cost at 500 iterations is: 355.59769752022976:\n",
      "Cost at 600 iterations is: 350.01339000624205:\n",
      "Cost at 700 iterations is: 344.5167822508214:\n",
      "Cost at 800 iterations is: 339.106496957282:\n",
      "Cost at 900 iterations is: 333.7811784589415:\n",
      "Cost at 1000 iterations is: 328.5394923794302:\n",
      "Cost at 1100 iterations is: 323.3801252983334:\n",
      "Cost at 1200 iterations is: 318.30178442208273:\n",
      "Cost at 1300 iterations is: 313.30319726002165:\n",
      "Cost at 1400 iterations is: 308.3831113055514:\n",
      "Cost at 1500 iterations is: 303.54029372228985:\n",
      "Cost at 1600 iterations is: 298.77353103515617:\n",
      "Cost at 1700 iterations is: 294.0816288263075:\n",
      "Cost at 1800 iterations is: 289.4634114358522:\n",
      "Cost at 1900 iterations is: 284.9177216672618:\n",
      "Cost at 2000 iterations is: 280.4434204974085:\n",
      "Cost at 2100 iterations is: 276.0393867911598:\n",
      "Cost at 2200 iterations is: 271.7045170204516:\n",
      "Cost at 2300 iterations is: 267.4377249877757:\n",
      "Cost at 2400 iterations is: 263.237941554009:\n",
      "Cost at 2500 iterations is: 259.1041143705161:\n",
      "Cost at 2600 iterations is: 255.03520761546105:\n",
      "Cost at 2700 iterations is: 251.03020173425838:\n",
      "Cost at 2800 iterations is: 247.08809318410277:\n",
      "Cost at 2900 iterations is: 243.2078941825069:\n",
      "Cost at 3000 iterations is: 239.38863245979195:\n",
      "Cost at 3100 iterations is: 235.62935101546432:\n",
      "Cost at 3200 iterations is: 231.92910787841674:\n",
      "Cost at 3300 iterations is: 228.28697587089746:\n",
      "Cost at 3400 iterations is: 224.70204237618537:\n",
      "Cost at 3500 iterations is: 221.17340910991334:\n",
      "Cost at 3600 iterations is: 217.70019189498234:\n",
      "Cost at 3700 iterations is: 214.2815204400127:\n",
      "Cost at 3800 iterations is: 210.91653812127365:\n",
      "Cost at 3900 iterations is: 207.60440176803476:\n",
      "Cost at 4000 iterations is: 204.34428145129402:\n",
      "Cost at 4100 iterations is: 201.13536027581796:\n",
      "Cost at 4200 iterations is: 197.9768341754529:\n",
      "Cost at 4300 iterations is: 194.86791171164649:\n",
      "Cost at 4400 iterations is: 191.807813875136:\n",
      "Cost at 4500 iterations is: 188.79577389075047:\n",
      "Cost at 4600 iterations is: 185.83103702527757:\n",
      "Cost at 4700 iterations is: 182.91286039834978:\n",
      "Cost at 4800 iterations is: 180.04051279629772:\n",
      "Cost at 4900 iterations is: 177.21327448892893:\n",
      "Cost at 5000 iterations is: 174.4304370491842:\n",
      "Cost at 5100 iterations is: 171.69130317562434:\n",
      "Cost at 5200 iterations is: 168.9951865177071:\n",
      "Cost at 5300 iterations is: 166.34141150380566:\n",
      "Cost at 5400 iterations is: 163.72931317193002:\n",
      "Cost at 5500 iterations is: 161.1582370031055:\n",
      "Cost at 5600 iterations is: 158.62753875736936:\n",
      "Cost at 5700 iterations is: 156.1365843123408:\n",
      "Cost at 5800 iterations is: 153.6847495043289:\n",
      "Cost at 5900 iterations is: 151.27141997193422:\n",
      "Cost at 6000 iterations is: 148.89599100210634:\n",
      "Cost at 6100 iterations is: 146.5578673786202:\n",
      "Cost at 6200 iterations is: 144.2564632329322:\n",
      "Cost at 6300 iterations is: 141.99120189737548:\n",
      "Cost at 6400 iterations is: 139.76151576066616:\n",
      "Cost at 6500 iterations is: 137.56684612567295:\n",
      "Cost at 6600 iterations is: 135.40664306942523:\n",
      "Cost at 6700 iterations is: 133.28036530531716:\n",
      "Cost at 6800 iterations is: 131.18748004747508:\n",
      "Cost at 6900 iterations is: 129.12746287725795:\n",
      "Cost at 7000 iterations is: 127.09979761185203:\n",
      "Cost at 7100 iterations is: 125.10397617492944:\n",
      "Cost at 7200 iterations is: 123.13949846933875:\n",
      "Cost at 7300 iterations is: 121.20587225179459:\n",
      "Cost at 7400 iterations is: 119.30261300953495:\n",
      "Cost at 7500 iterations is: 117.42924383891595:\n",
      "Cost at 7600 iterations is: 115.58529532591338:\n",
      "Cost at 7700 iterations is: 113.77030542849988:\n",
      "Cost at 7800 iterations is: 111.9838193608706:\n",
      "Cost at 7900 iterations is: 110.22538947948662:\n",
      "Cost at 8000 iterations is: 108.49457517090767:\n",
      "Cost at 8100 iterations is: 106.79094274138656:\n",
      "Cost at 8200 iterations is: 105.11406530819826:\n",
      "Cost at 8300 iterations is: 103.46352269267425:\n",
      "Cost at 8400 iterations is: 101.83890131491768:\n",
      "Cost at 8500 iterations is: 100.23979409017197:\n",
      "Cost at 8600 iterations is: 98.66580032681682:\n",
      "Cost at 8700 iterations is: 97.11652562596514:\n",
      "Cost at 8800 iterations is: 95.59158178263891:\n",
      "Cost at 8900 iterations is: 94.09058668849524:\n",
      "Cost at 9000 iterations is: 92.6131642360803:\n",
      "Cost at 9100 iterations is: 91.15894422458759:\n",
      "Cost at 9200 iterations is: 89.72756226709699:\n",
      "Cost at 9300 iterations is: 88.318659699267:\n",
      "Cost at 9400 iterations is: 86.93188348946619:\n",
      "Cost at 9500 iterations is: 85.56688615031159:\n",
      "Cost at 9600 iterations is: 84.22332565159878:\n",
      "Cost at 9700 iterations is: 82.90086533459842:\n",
      "Cost at 9800 iterations is: 81.59917382769854:\n",
      "Cost at 9900 iterations is: 80.31792496337304:\n",
      "final result of w [0.08237526 0.53552137 0.02752216] final result of b 0.01056185052956357\n"
     ]
    }
   ],
   "source": [
    "iteration = 10000\n",
    "alpha=1e-7\n",
    "w = np.zeros(X.shape[1])\n",
    "w_model,b_model = gradient_descent(X,y,w,b,iteration,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the value of w, [0.08 0.54 0.03] and b is 0.0106.This implies the model after fitting/training is:\n",
    "\n",
    "0.08x + 0.54x**2 + 0.03x**3 + 0.0106\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review this idea:\n",
    "- Intially, the features were re-scaled so they are comparable to each other\n",
    "- less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature useful in fitting the model to the data.\n",
    "- above, after fitting, the weight associated with the $x^2$ feature is much larger than the weights for $x$ or $x^3$ as it is the most useful in fitting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn linear regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,SGDRegressor # gradient descent regression model\n",
    "from sklearn.preprocessing import StandardScaler # This will perform z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:,:4]\n",
    "    y = data[:,4]\n",
    "    return X, y\n",
    "X,y = load_house_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:[2.406e+03 4.000e+00 1.000e+00 9.500e+01]\n",
      "Peak to Peak range by column in Normalized X:[5.8452591  6.13529646 2.05626214 3.68533012]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create and fit the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor()\n",
      "number of iterations completed: 123, number of weight updates: 12178.0\n"
     ]
    }
   ],
   "source": [
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the parameters are associated with the *normalized* input data. The fit parameters are very close to those found in the above example with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:                   w: [110.11738399 -21.0391742  -32.4421159  -38.04502188], b:[363.15841409]\n",
      "model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\n"
     ]
    }
   ],
   "source": [
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")\n",
    "print(f\"model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the targets of the training data. Use both the `predict` routine and compute using $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction using np.dot() and sgdr.predict match: True\n",
      "Prediction on training set:\n",
      "[295.18145756 485.9081823  389.62071486 492.07023692]\n",
      "Target values \n",
      "[300.  509.8 394.  540. ]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction using sgdr.predict()\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "# make a prediction using w,b. \n",
    "y_pred = np.dot(X_norm, w_norm) + b_norm  \n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "print(f\"Target values \\n{y[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression, Closed-form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closed form solution is a formula that can be used to find the optimal parameters for a linear regression model without using an iterative algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300, 500])   #target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "#X must be a 2-D Matrix/\n",
    "linear_model.fit(X_train.reshape(-1, 1), y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Parameters \n",
    "The $\\mathbf{w}$ and $\\mathbf{b}$ parameters are referred to as 'coefficients' and 'intercept' in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [200.], b = 100.00\n",
      "'manual' prediction: f_wb = wx+b : [240100.]\n"
     ]
    }
   ],
   "source": [
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")\n",
    "print(f\"'manual' prediction: f_wb = wx+b : {1200*w + b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Calling the `predict` function generates predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set: [300. 500.]\n",
      "Prediction for 1200 sqft house: $240100.00\n"
     ]
    }
   ],
   "source": [
    "y_pred = linear_model.predict(X_train.reshape(-1, 1))\n",
    "\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "\n",
    "X_test = np.array([[1200]])\n",
    "print(f\"Prediction for 1200 sqft house: ${linear_model.predict(X_test)[0]:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Example\n",
    "The second example is from an earlier lab with multiple features. The final parameter values and predictions are very close to the results from the un-normalized 'long-run' from that lab. That un-normalized run took hours to produce results, while this is nearly instantaneous. The closed-form solution work well on smaller data sets such as these but can be computationally demanding on larger data sets. \n",
    ">The closed-form solution does not require normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [  0.26860107 -32.62006902 -67.25453872  -1.47297443], b = 220.42\n"
     ]
    }
   ],
   "source": [
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set:\n",
      " [295.17615301 485.97796332 389.52416548 492.14712499]\n",
      "prediction using w,b:\n",
      " [295.17615301 485.97796332 389.52416548 492.14712499]\n",
      "Target values \n",
      " [300.  509.8 394.  540. ]\n",
      " predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318709.09\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction on training set:\\n {linear_model.predict(X)[:4]}\" )\n",
    "print(f\"prediction using w,b:\\n {(X @ w + b)[:4]}\")\n",
    "print(f\"Target values \\n {y[:4]}\")\n",
    "\n",
    "x_house = np.array([1200, 3,1, 40]).reshape(-1,4)\n",
    "x_house_predict = linear_model.predict(x_house)[0]\n",
    "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d23d88b1f690fc172c8599d0a72479bc1e1fe14537353f673716243eaedacc28"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
